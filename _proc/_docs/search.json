[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "generative_models",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "generative_models",
    "section": "Install",
    "text": "Install\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "generative_models",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Autoencoder",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn"
  },
  {
    "objectID": "core.html#data",
    "href": "core.html#data",
    "title": "Autoencoder",
    "section": "Data",
    "text": "Data\n\nMNNIST\n\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nmnist_train = datasets.MNIST(root=\"datasets\", train=True, \n                             download=True, transform=ToTensor())\nmnist_val = datasets.MNIST(root=\"datasets\", train=False,\n                           transform=ToTensor())\nmnist_train, mnist_val\n\n(Dataset MNIST\n     Number of datapoints: 60000\n     Root location: datasets\n     Split: Train\n     StandardTransform\n Transform: ToTensor(),\n Dataset MNIST\n     Number of datapoints: 10000\n     Root location: datasets\n     Split: Test\n     StandardTransform\n Transform: ToTensor())\n\n\n\nd0 = mnist_train[0]\n\n\n[type(x) for x in d0]\n\n[torch.Tensor, int]\n\n\n\nd0[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Print attributes\n[s for s in dir(mnist_train) if not s.startswith(\"_\")]\n\n['class_to_idx',\n 'classes',\n 'data',\n 'download',\n 'extra_repr',\n 'mirrors',\n 'processed_folder',\n 'raw_folder',\n 'resources',\n 'root',\n 'target_transform',\n 'targets',\n 'test_data',\n 'test_file',\n 'test_labels',\n 'train',\n 'train_data',\n 'train_labels',\n 'training_file',\n 'transform',\n 'transforms']\n\n\n\nmnist_train.class_to_idx\n\n{'0 - zero': 0,\n '1 - one': 1,\n '2 - two': 2,\n '3 - three': 3,\n '4 - four': 4,\n '5 - five': 5,\n '6 - six': 6,\n '7 - seven': 7,\n '8 - eight': 8,\n '9 - nine': 9}\n\n\n\nmnist_loaders = {\n    \"train\": torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True),\n    \"val\": torch.utils.data.DataLoader(mnist_val, batch_size=32, shuffle=False),\n}\n\n\ndl0 = next(iter(mnist_loaders[\"train\"]))\ndl0[0].shape\n\ntorch.Size([32, 1, 28, 28])\n\n\n\n# Show a batch of images\n\nbatch = next(iter(mnist_loaders[\"train\"]))\nimages, labels = batch[:15]\nfig, axes = plt.subplots(3, 5, figsize=(10, 6))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n    ax.set_title(labels[i].item())\n    ax.axis(\"off\")\n\n\n\n\n\n\nFashion MNIST\n\n# TODO"
  },
  {
    "objectID": "core.html#autoencoder",
    "href": "core.html#autoencoder",
    "title": "Autoencoder",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nfrom models import BabyAutoEncoder\n\n# ae_model = BabyAutoEncoder()"
  },
  {
    "objectID": "autoencoder.html",
    "href": "autoencoder.html",
    "title": "Autoencoder",
    "section": "",
    "text": "source\n\nBabyAutoEncoder\n\n BabyAutoEncoder (n_in, n_latent, n_out)\n\nPretty much the simplest autoencoder you can imagine.\n\nn_in = n_out = 64\nn_latent = 4\n\nmodel = BabyAutoEncoder(n_in, n_latent, n_out)\n\nx = torch.rand(1, n_in)\n\nout = model(x)\ntest_eq(out.shape, (1, n_out))"
  },
  {
    "objectID": "autoencoder.html#data",
    "href": "autoencoder.html#data",
    "title": "generative_models",
    "section": "Data",
    "text": "Data\n\nMNNIST\n\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nmnist_train = datasets.MNIST(root=\"datasets\", train=True, \n                             download=True, transform=ToTensor())\nmnist_val = datasets.MNIST(root=\"datasets\", train=False,\n                           transform=ToTensor())\nmnist_train, mnist_val\n\n(Dataset MNIST\n     Number of datapoints: 60000\n     Root location: datasets\n     Split: Train\n     StandardTransform\n Transform: ToTensor(),\n Dataset MNIST\n     Number of datapoints: 10000\n     Root location: datasets\n     Split: Test\n     StandardTransform\n Transform: ToTensor())\n\n\n\nd0 = mnist_train[0]\n\n\n[type(x) for x in d0]\n\n[torch.Tensor, int]\n\n\n\nd0[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Print attributes\n[s for s in dir(mnist_train) if not s.startswith(\"_\")]\n\n['class_to_idx',\n 'classes',\n 'data',\n 'download',\n 'extra_repr',\n 'mirrors',\n 'processed_folder',\n 'raw_folder',\n 'resources',\n 'root',\n 'target_transform',\n 'targets',\n 'test_data',\n 'test_file',\n 'test_labels',\n 'train',\n 'train_data',\n 'train_labels',\n 'training_file',\n 'transform',\n 'transforms']\n\n\n\nmnist_train.class_to_idx\n\n{'0 - zero': 0,\n '1 - one': 1,\n '2 - two': 2,\n '3 - three': 3,\n '4 - four': 4,\n '5 - five': 5,\n '6 - six': 6,\n '7 - seven': 7,\n '8 - eight': 8,\n '9 - nine': 9}\n\n\n\nmnist_loaders = {\n    \"train\": torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True),\n    \"val\": torch.utils.data.DataLoader(mnist_val, batch_size=32, shuffle=False),\n}\n\n\ndl0 = next(iter(mnist_loaders[\"train\"]))\ndl0[0].shape\n\ntorch.Size([32, 1, 28, 28])\n\n\n\n# Show a batch of images\n\nbatch = next(iter(mnist_loaders[\"train\"]))\nimages, labels = batch[:15]\nfig, axes = plt.subplots(3, 5, figsize=(10, 6))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n    ax.set_title(labels[i].item())\n    ax.axis(\"off\")\n\n\n\n\n\n\nFashion MNIST\n\n# TODO"
  },
  {
    "objectID": "autoencoder.html#autoencoder",
    "href": "autoencoder.html#autoencoder",
    "title": "generative_models",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nfrom models import BabyAutoEncoder\n\n# ae_model = BabyAutoEncoder()"
  },
  {
    "objectID": "01_autoencoder.html",
    "href": "01_autoencoder.html",
    "title": "generative_models",
    "section": "",
    "text": "# Autoencoder\n\n> Autoencoder"
  },
  {
    "objectID": "01_autoencoder.html#data",
    "href": "01_autoencoder.html#data",
    "title": "generative_models",
    "section": "Data",
    "text": "Data\n\nMNNIST\n\nmnist_train = datasets.MNIST(root=\"datasets\", train=True, \n                             download=True, transform=ToTensor())\nmnist_val = datasets.MNIST(root=\"datasets\", train=False,\n                           transform=ToTensor())\nmnist_train, mnist_val\n\n(Dataset MNIST\n     Number of datapoints: 60000\n     Root location: datasets\n     Split: Train\n     StandardTransform\n Transform: ToTensor(),\n Dataset MNIST\n     Number of datapoints: 10000\n     Root location: datasets\n     Split: Test\n     StandardTransform\n Transform: ToTensor())\n\n\n\nd0 = mnist_train[0]\n\n\n[type(x) for x in d0]\n\n[torch.Tensor, int]\n\n\n\nd0[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Print attributes\n[s for s in dir(mnist_train) if not s.startswith(\"_\")]\n\n['class_to_idx',\n 'classes',\n 'data',\n 'download',\n 'extra_repr',\n 'mirrors',\n 'processed_folder',\n 'raw_folder',\n 'resources',\n 'root',\n 'target_transform',\n 'targets',\n 'test_data',\n 'test_file',\n 'test_labels',\n 'train',\n 'train_data',\n 'train_labels',\n 'training_file',\n 'transform',\n 'transforms']\n\n\n\nmnist_train.class_to_idx\n\n{'0 - zero': 0,\n '1 - one': 1,\n '2 - two': 2,\n '3 - three': 3,\n '4 - four': 4,\n '5 - five': 5,\n '6 - six': 6,\n '7 - seven': 7,\n '8 - eight': 8,\n '9 - nine': 9}\n\n\n\nmnist_loaders = {\n    \"train\": torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True),\n    \"val\": torch.utils.data.DataLoader(mnist_val, batch_size=32, shuffle=False),\n}\n\n\ndl0 = next(iter(mnist_loaders[\"train\"]))\ndl0[0].shape\n\ntorch.Size([32, 1, 28, 28])\n\n\n\n# Show a batch of images\n\nbatch = next(iter(mnist_loaders[\"train\"]))\nimages, labels = batch[:15]\nfig, axes = plt.subplots(3, 5, figsize=(10, 6))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n    ax.set_title(labels[i].item())\n    ax.axis(\"off\")\n\n\n\n\n\n\nFashion MNIST\n\n# TODO"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "source\n\n\n\n load_mnist ()\n\n\nmnist_train, mnist_val = load_mnist()\nmnist_train, mnist_val\n\n(Dataset MNIST\n     Number of datapoints: 60000\n     Root location: datasets\n     Split: Train\n     StandardTransform\n Transform: ToTensor(),\n Dataset MNIST\n     Number of datapoints: 10000\n     Root location: datasets\n     Split: Test\n     StandardTransform\n Transform: ToTensor())\n\n\nShow the first batch from the training dataloader:\n\nd0 = mnist_train[0]\n\nThe output is a tuple containing a tensor and a class index:\n\n[type(x) for x in d0]\n\n[torch.Tensor, int]\n\n\nWhere the image is a 28x28 tensor.\n\nd0[0].shape\n\ntorch.Size([1, 28, 28])\n\n\nLet’s print the dataset attributes to get an idea of the MNIST torchvision.datasets.MNIST class\n\n# Print attributes\n[s for s in dir(mnist_train) if not s.startswith(\"_\")]\n\n['class_to_idx',\n 'classes',\n 'data',\n 'download',\n 'extra_repr',\n 'mirrors',\n 'processed_folder',\n 'raw_folder',\n 'resources',\n 'root',\n 'target_transform',\n 'targets',\n 'test_data',\n 'test_file',\n 'test_labels',\n 'train',\n 'train_data',\n 'train_labels',\n 'training_file',\n 'transform',\n 'transforms']\n\n\n\nmnist_train.class_to_idx\n\n{'0 - zero': 0,\n '1 - one': 1,\n '2 - two': 2,\n '3 - three': 3,\n '4 - four': 4,\n '5 - five': 5,\n '6 - six': 6,\n '7 - seven': 7,\n '8 - eight': 8,\n '9 - nine': 9}\n\n\nDefine the train and validation dataloaders:\n\nmnist_loaders = {\n    \"train\": torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True),\n    \"val\": torch.utils.data.DataLoader(mnist_val, batch_size=32, shuffle=False),\n}\n\n\ndl0 = next(iter(mnist_loaders[\"train\"]))\ndl0[0].shape\n\ntorch.Size([32, 1, 28, 28])\n\n\nNow visualize what a batch of images looks like.\n\nbatch = next(iter(mnist_loaders[\"train\"]))\nimages, labels = batch[:15]\nfig, axes = plt.subplots(3, 5, figsize=(10, 6))\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n    ax.set_title(labels[i].item())\n    ax.axis(\"off\")\n\n\n\n\n\n\n\n\n# TODO"
  }
]